{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using this notebook\n",
    "The code is highly modularised, which means if you are interested in  directly viewing the results, you can skip most of the initial parts of the notebook and start from the section \"baseline model\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements/ changes from the last attempt:\n",
    " - separate models for both the cities\n",
    " - smaller and flexible functions for data preprocesing to facilitate easy data reorganisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importing the relevant packages and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"data/dengue_features_train.csv\")\n",
    "y = pd.read_csv(\"data/dengue_labels_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(X):\n",
    "    \n",
    "    # remove the column that has ~20% null values, also ranks low on feature importance\n",
    "    X.drop(['ndvi_ne'], axis=1, inplace=True)\n",
    "    \n",
    "    # Filling the rest using linear interpolation\n",
    "    X.interpolate(inplace=True)\n",
    "\n",
    "def remove_outliers(df):\n",
    "    return df[(np.abs(stats.zscore(df)) < 5).all(axis=1)]\n",
    "\n",
    "def mape(Y_test, Y_pred, epsilon = 1):\n",
    "    return np.mean(np.abs((Y_test - Y_pred + epsilon) / (Y_test + epsilon))) * 100\n",
    "\n",
    "def extract_month(s):\n",
    "    return int(s[5:7])\n",
    "\n",
    "def city_indices(X):\n",
    "    # city boolean encoding\n",
    "    return X.city == 'sj'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(X, trees = False):\n",
    "    \"\"\"\n",
    "    Extracts the month out of date and converts it to a one hot\n",
    "    Standardizes the numerical features\n",
    "    \"\"\"\n",
    "    \n",
    "    #Extracting month from the date\n",
    "    months = X.week_start_date.apply(extract_month)\n",
    "\n",
    "    # Removing the columns not required for classification\n",
    "    X.drop(['city', 'year', 'weekofyear', 'week_start_date'], axis=1, inplace=True)\n",
    "\n",
    "    # Standardizing the data\n",
    "    if not trees:\n",
    "        scaler = StandardScaler()\n",
    "        X[X.columns] = scaler.fit_transform(X)\n",
    "\n",
    "    # Month one hot features\n",
    "    month_features = pd.get_dummies(months)\n",
    "    X = X.join(month_features)\n",
    "\n",
    "    # Alternatively use months as a discrete feature\n",
    "    #X = X.join(months)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def seperate_cities_data(X, is_sj):\n",
    "    \"\"\"\n",
    "    Separating the data for the two cities in order to create a different model for each\n",
    "    \"\"\"\n",
    "\n",
    "    # Separating the cities data\n",
    "    X_sj = X.loc[is_sj]\n",
    "    X_iq = X.loc[~is_sj]\n",
    "    \n",
    "    return X_sj, X_iq\n",
    "\n",
    "def get_y_labels(X_sj, X_iq, y):    \n",
    "    \n",
    "    y = y.total_cases    \n",
    "    y_sj = y.loc[X_sj.index]\n",
    "    y_iq = y.loc[X_iq.index]\n",
    "    \n",
    "    return y_sj, y_iq\n",
    "\n",
    "def split(X_sj, X_iq, y_sj, y_iq):\n",
    "\n",
    "    # train and test split\n",
    "    sj_split_data = train_test_split(X_sj, y_sj, shuffle = False)\n",
    "    iq_split_data = train_test_split(X_iq, y_iq, shuffle = False)\n",
    "\n",
    "    return sj_split_data, iq_split_data\n",
    "\n",
    "def process(X, y = pd.Series(), train = True, trees = False):\n",
    "    \"\"\"\n",
    "    preprocesses the data\n",
    "    \n",
    "    arguements:\n",
    "    X - the data to be pre processed\n",
    "    y - labels, optional\n",
    "    train - if False, computes pre processing for test data set.\n",
    "    tress - True if the model to be used is a tree based one, for eg. XGBoost\n",
    "    \"\"\"\n",
    "    \n",
    "    is_sj = city_indices(X)\n",
    "    if not trees:\n",
    "        impute(X)\n",
    "    X = pre_process(X, trees)\n",
    "    \n",
    "    #X = remove_outliers(X)\n",
    "    X_sj, X_iq = seperate_cities_data(X, is_sj)\n",
    "    if y.empty:\n",
    "        return X_sj, X_iq\n",
    "    \n",
    "    y_sj, y_iq = get_y_labels(X_sj, X_iq, y)\n",
    "    if not train:\n",
    "        return X_sj, X_iq, y_sj, y_iq\n",
    "    \n",
    "    return split(X_sj, X_iq, y_sj, y_iq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process(X,y)\n",
    "(X_sj_train, X_sj_test, Y_sj_train, Y_sj_test), (X_iq_train, X_iq_test, Y_iq_train, Y_iq_test) = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random(Y_test, Y_train):\n",
    "    y_p = np.full(len(Y_test), np.mean(Y_train))\n",
    "    return mean_absolute_error(Y_test, y_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The performance on random models shall guide us on improving our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.027284681130833"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random(Y_sj_test, Y_sj_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.287337278106508"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random(Y_iq_test, Y_iq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y_sj_test.append(Y_iq_test)\n",
    "Y_train = Y_sj_train.append(Y_iq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.337127158555734"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random(Y_test, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalised Model\n",
    "A generalised model is nothing but the combination of accuracy scores on different cities trained on different models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_model(clf, data):\n",
    "    \"\"\"\n",
    "    returns the mean absolute error combined for both the cities.\n",
    "    \"\"\"\n",
    "    (X_sj_train, X_sj_test, Y_sj_train, Y_sj_test), (X_iq_train, X_iq_test, Y_iq_train, Y_iq_test) = data\n",
    "    \n",
    "    clf.fit(X_sj_train, Y_sj_train, eval_metric = mean_absolute_error)\n",
    "    Y_sj_pred = clf.predict(X_sj_test)\n",
    "    \n",
    "    clf.fit(X_iq_train, Y_iq_train, eval_metric = mean_absolute_error)\n",
    "    Y_iq_pred = clf.predict(X_iq_test)\n",
    "    \n",
    "    Y_pred = np.concatenate([Y_sj_pred, Y_iq_pred])\n",
    "    Y_pred = Y_pred.astype(int).clip(0)\n",
    "    Y_test = Y_sj_test.append(Y_iq_test)\n",
    "    \n",
    "    return mean_absolute_error(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"data/dengue_features_train.csv\")\n",
    "y = pd.read_csv(\"data/dengue_labels_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = process(X,y, trees=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.26923076923077"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBRegressor()\n",
    "general_model(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A simple XGBoost regressor gives slight improvement on the baseline but nothing significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.39010989010989"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we find gamma=0 and max_depth=3, defaut parameters to be best, rest tuned parameters can be seen below\n",
    "rf = XGBRegressor(n_estimators=55, learning_rate=0.01 ,n_jobs=-1, subsample=0.9, colsample_bytree=0.6, colsample_bylevel=0.1, min_child_weight=5, reg_alpha=0.1) \n",
    "\n",
    "general_model(rf, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A tuned XGB model shows some significant improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross validation scores\n",
    "To examine things more closely, let us view the scores on cross-validations, we should ideally use time-based cross-validations, since we do not have enough data, let us for once just go for the regular thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30.59434104, 49.11759507, 14.95902743, 32.26011295, 12.66011701])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = XGBRegressor(n_estimators=55, learning_rate=0.01 ,n_jobs=-1, subsample=0.9, colsample_bytree=0.6, colsample_bylevel=0.1, min_child_weight=5, reg_alpha=0.1) \n",
    "(X_sj_train, X_sj_test, Y_sj_train, Y_sj_test), (X_iq_train, X_iq_test, Y_iq_train, Y_iq_test) = data\n",
    "cross_val_score(rf, X_sj_train, Y_sj_train, cv = 5, scoring=make_scorer(mean_absolute_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.25106094, 7.87761068, 7.00398358, 5.8487702 , 3.53553   ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf, X_iq_train, Y_iq_train, cv = 5, scoring=make_scorer(mean_absolute_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is too much variation across cross-validations, we need to rethink our modelling approches"
   ]
  },
 
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also have a look at **time-based cross-validations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBRegressor(n_estimators=55, learning_rate=0.01 ,n_jobs=-1, subsample=0.9, colsample_bytree=0.6, colsample_bylevel=0.1, min_child_weight=5, reg_alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40.78982837, 31.14011517, 12.58860924])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_splits = TimeSeriesSplit(n_splits=3).split(X_sj_train, Y_sj_train)\n",
    "\n",
    "cross_val_score(clf, X_sj_train, Y_sj_train, cv=cv_splits, scoring=make_scorer(mean_absolute_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.49918169, 8.7481604 , 4.26543863])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_splits = TimeSeriesSplit(n_splits=3).split(X_iq_train, Y_iq_train)\n",
    "cross_val_score(clf, X_iq_train, Y_iq_train, cv=cv_splits, scoring=make_scorer(mean_absolute_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion : \n",
    "XGBoost does perform good on a tuned model but a closer examination of cross-validation scores reveals a huge variance in the results, the improvement in the accuracy of the tuned model is likely the result of overfitting to one train-test distribution.\n",
    " \n",
    "#### Next Steps :\n",
    "We are going to need to take a closer look at the distribution of our data and based on that choose some more advanced statistical models, perhaps outside of the standard libraries like the scikit learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
